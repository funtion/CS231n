{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000L, 3073L)\n",
      "Train labels shape:  (49000L,)\n",
      "Validation data shape:  (1000L, 3073L)\n",
      "Validation labels shape:  (1000L,)\n",
      "Test data shape:  (1000L, 3073L)\n",
      "Test labels shape:  (1000L,)\n",
      "dev data shape:  (500L, 3073L)\n",
      "dev labels shape:  (500L,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.376542\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n",
    "1/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.765259 analytic: -1.765259, relative error: 2.315703e-08\n",
      "numerical: -0.183276 analytic: -0.183276, relative error: 5.014279e-07\n",
      "numerical: -2.128916 analytic: -2.128916, relative error: 2.309714e-08\n",
      "numerical: 0.930324 analytic: 0.930324, relative error: 2.535893e-08\n",
      "numerical: -1.703008 analytic: -1.703008, relative error: 2.195330e-08\n",
      "numerical: 1.155027 analytic: 1.155027, relative error: 8.315630e-08\n",
      "numerical: 0.461581 analytic: 0.461581, relative error: 3.617503e-08\n",
      "numerical: -2.686594 analytic: -2.686594, relative error: 1.579622e-08\n",
      "numerical: -0.498198 analytic: -0.498198, relative error: 1.451138e-08\n",
      "numerical: 1.487932 analytic: 1.487932, relative error: 3.543010e-10\n",
      "----\n",
      "numerical: -1.294409 analytic: -1.294409, relative error: 3.276653e-08\n",
      "numerical: -0.279368 analytic: -0.279368, relative error: 6.679729e-08\n",
      "numerical: 0.059869 analytic: 0.059869, relative error: 2.010021e-07\n",
      "numerical: 1.713864 analytic: 1.713864, relative error: 5.902133e-08\n",
      "numerical: 0.118880 analytic: 0.118880, relative error: 3.486782e-07\n",
      "numerical: 0.714986 analytic: 0.714986, relative error: 4.412617e-08\n",
      "numerical: -1.957110 analytic: -1.957110, relative error: 2.124969e-08\n",
      "numerical: 0.091388 analytic: 0.091388, relative error: 1.666560e-07\n",
      "numerical: 1.625371 analytic: 1.625371, relative error: 2.580569e-09\n",
      "numerical: 1.882934 analytic: 1.882934, relative error: 2.600986e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "print \"----\"\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.376542e+00 computed in 0.235000s\n",
      "vectorized loss: 2.376542e+00 computed in 0.005000s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 5.985225\n",
      "iteration 100 / 2000: loss 5.724078\n",
      "iteration 200 / 2000: loss 5.933475\n",
      "iteration 300 / 2000: loss 5.630185\n",
      "iteration 400 / 2000: loss 5.770739\n",
      "iteration 500 / 2000: loss 6.156790\n",
      "iteration 600 / 2000: loss 6.259621\n",
      "iteration 700 / 2000: loss 5.678629\n",
      "iteration 800 / 2000: loss 6.057438\n",
      "iteration 900 / 2000: loss 6.082555\n",
      "iteration 1000 / 2000: loss 5.817058\n",
      "iteration 1100 / 2000: loss 5.575862\n",
      "iteration 1200 / 2000: loss 6.029535\n",
      "iteration 1300 / 2000: loss 5.642295\n",
      "iteration 1400 / 2000: loss 6.307583\n",
      "iteration 1500 / 2000: loss 5.905111\n",
      "iteration 1600 / 2000: loss 5.967488\n",
      "iteration 1700 / 2000: loss 6.130946\n",
      "iteration 1800 / 2000: loss 5.968412\n",
      "iteration 1900 / 2000: loss 5.549293\n",
      "iteration 0 / 2000: loss 6.467978\n",
      "iteration 100 / 2000: loss 6.991222\n",
      "iteration 200 / 2000: loss 5.893888\n",
      "iteration 300 / 2000: loss 5.446675\n",
      "iteration 400 / 2000: loss 6.526036\n",
      "iteration 500 / 2000: loss 6.622491\n",
      "iteration 600 / 2000: loss 6.015822\n",
      "iteration 700 / 2000: loss 6.680837\n",
      "iteration 800 / 2000: loss 6.473238\n",
      "iteration 900 / 2000: loss 6.328748\n",
      "iteration 1000 / 2000: loss 7.100600\n",
      "iteration 1100 / 2000: loss 6.853261\n",
      "iteration 1200 / 2000: loss 6.350873\n",
      "iteration 1300 / 2000: loss 6.038403\n",
      "iteration 1400 / 2000: loss 6.253582\n",
      "iteration 1500 / 2000: loss 6.005228\n",
      "iteration 1600 / 2000: loss 6.199228\n",
      "iteration 1700 / 2000: loss 6.192047\n",
      "iteration 1800 / 2000: loss 5.860038\n",
      "iteration 1900 / 2000: loss 5.958026\n",
      "iteration 0 / 2000: loss 5.809548\n",
      "iteration 100 / 2000: loss 5.592789\n",
      "iteration 200 / 2000: loss 5.989567\n",
      "iteration 300 / 2000: loss 5.716281\n",
      "iteration 400 / 2000: loss 5.929581\n",
      "iteration 500 / 2000: loss 6.109076\n",
      "iteration 600 / 2000: loss 5.379948\n",
      "iteration 700 / 2000: loss 6.605628\n",
      "iteration 800 / 2000: loss 5.793859\n",
      "iteration 900 / 2000: loss 5.941958\n",
      "iteration 1000 / 2000: loss 5.856205\n",
      "iteration 1100 / 2000: loss 6.020277\n",
      "iteration 1200 / 2000: loss 6.312988\n",
      "iteration 1300 / 2000: loss 5.648092\n",
      "iteration 1400 / 2000: loss 5.845626\n",
      "iteration 1500 / 2000: loss 5.988038\n",
      "iteration 1600 / 2000: loss 5.712547\n",
      "iteration 1700 / 2000: loss 6.057829\n",
      "iteration 1800 / 2000: loss 5.789704\n",
      "iteration 1900 / 2000: loss 5.942616\n",
      "iteration 0 / 2000: loss 5.295482\n",
      "iteration 100 / 2000: loss 4.765403\n",
      "iteration 200 / 2000: loss 4.599121\n",
      "iteration 300 / 2000: loss 4.426085\n",
      "iteration 400 / 2000: loss 5.068253\n",
      "iteration 500 / 2000: loss 4.901475\n",
      "iteration 600 / 2000: loss 4.712057\n",
      "iteration 700 / 2000: loss 4.704271\n",
      "iteration 800 / 2000: loss 4.938772\n",
      "iteration 900 / 2000: loss 4.542968\n",
      "iteration 1000 / 2000: loss 4.820984\n",
      "iteration 1100 / 2000: loss 4.763402\n",
      "iteration 1200 / 2000: loss 5.090221\n",
      "iteration 1300 / 2000: loss 4.884414\n",
      "iteration 1400 / 2000: loss 4.496082\n",
      "iteration 1500 / 2000: loss 5.051280\n",
      "iteration 1600 / 2000: loss 5.069095\n",
      "iteration 1700 / 2000: loss 4.743732\n",
      "iteration 1800 / 2000: loss 4.986480\n",
      "iteration 1900 / 2000: loss 4.447693\n",
      "iteration 0 / 2000: loss 6.069333\n",
      "iteration 100 / 2000: loss 5.718335\n",
      "iteration 200 / 2000: loss 5.777570\n",
      "iteration 300 / 2000: loss 5.634358\n",
      "iteration 400 / 2000: loss 5.561568\n",
      "iteration 500 / 2000: loss 5.627246\n",
      "iteration 600 / 2000: loss 5.526892\n",
      "iteration 700 / 2000: loss 5.991781\n",
      "iteration 800 / 2000: loss 5.756690\n",
      "iteration 900 / 2000: loss 5.497288\n",
      "iteration 1000 / 2000: loss 5.909765\n",
      "iteration 1100 / 2000: loss 6.144632\n",
      "iteration 1200 / 2000: loss 6.100802\n",
      "iteration 1300 / 2000: loss 6.330141\n",
      "iteration 1400 / 2000: loss 6.102057\n",
      "iteration 1500 / 2000: loss 5.857822\n",
      "iteration 1600 / 2000: loss 5.400557\n",
      "iteration 1700 / 2000: loss 5.949271\n",
      "iteration 1800 / 2000: loss 5.911138\n",
      "iteration 1900 / 2000: loss 5.672244\n",
      "iteration 0 / 2000: loss 6.820979\n",
      "iteration 100 / 2000: loss 6.581629\n",
      "iteration 200 / 2000: loss 6.566944\n",
      "iteration 300 / 2000: loss 7.049330\n",
      "iteration 400 / 2000: loss 6.586334\n",
      "iteration 500 / 2000: loss 6.850007\n",
      "iteration 600 / 2000: loss 6.519657\n",
      "iteration 700 / 2000: loss 6.360447\n",
      "iteration 800 / 2000: loss 6.326872\n",
      "iteration 900 / 2000: loss 6.735034\n",
      "iteration 1000 / 2000: loss 7.170115\n",
      "iteration 1100 / 2000: loss 6.656112\n",
      "iteration 1200 / 2000: loss 6.735894\n",
      "iteration 1300 / 2000: loss 6.968040\n",
      "iteration 1400 / 2000: loss 6.738109\n",
      "iteration 1500 / 2000: loss 7.144068\n",
      "iteration 1600 / 2000: loss 6.873746\n",
      "iteration 1700 / 2000: loss 6.936477\n",
      "iteration 1800 / 2000: loss 6.546165\n",
      "iteration 1900 / 2000: loss 7.002177\n",
      "iteration 0 / 2000: loss 21.565033\n",
      "iteration 100 / 2000: loss 21.263023\n",
      "iteration 200 / 2000: loss 20.639473\n",
      "iteration 300 / 2000: loss 21.496859\n",
      "iteration 400 / 2000: loss 21.365793\n",
      "iteration 500 / 2000: loss 21.545717\n",
      "iteration 600 / 2000: loss 21.023327\n",
      "iteration 700 / 2000: loss 21.531090\n",
      "iteration 800 / 2000: loss 21.032904\n",
      "iteration 900 / 2000: loss 21.296736\n",
      "iteration 1000 / 2000: loss 21.071764\n",
      "iteration 1100 / 2000: loss 21.346647\n",
      "iteration 1200 / 2000: loss 21.057742\n",
      "iteration 1300 / 2000: loss 21.864853\n",
      "iteration 1400 / 2000: loss 21.156018\n",
      "iteration 1500 / 2000: loss 20.965929\n",
      "iteration 1600 / 2000: loss 21.078780\n",
      "iteration 1700 / 2000: loss 21.038406\n",
      "iteration 1800 / 2000: loss 21.283204\n",
      "iteration 1900 / 2000: loss 20.953437\n",
      "iteration 0 / 2000: loss 158.503181\n",
      "iteration 100 / 2000: loss 158.513961\n",
      "iteration 200 / 2000: loss 157.780790\n",
      "iteration 300 / 2000: loss 158.020597\n",
      "iteration 400 / 2000: loss 158.309543\n",
      "iteration 500 / 2000: loss 158.431782\n",
      "iteration 600 / 2000: loss 158.328786\n",
      "iteration 700 / 2000: loss 158.312653\n",
      "iteration 800 / 2000: loss 158.188555\n",
      "iteration 900 / 2000: loss 157.841439\n",
      "iteration 1000 / 2000: loss 158.037206\n",
      "iteration 1100 / 2000: loss 157.709456\n",
      "iteration 1200 / 2000: loss 157.821700\n",
      "iteration 1300 / 2000: loss 157.753234\n",
      "iteration 1400 / 2000: loss 157.519802\n",
      "iteration 1500 / 2000: loss 157.451357\n",
      "iteration 1600 / 2000: loss 158.054373\n",
      "iteration 1700 / 2000: loss 157.351575\n",
      "iteration 1800 / 2000: loss 157.756010\n",
      "iteration 1900 / 2000: loss 157.399743\n",
      "iteration 0 / 2000: loss 1531.992935\n",
      "iteration 100 / 2000: loss 1528.613260\n",
      "iteration 200 / 2000: loss 1526.123039\n",
      "iteration 300 / 2000: loss 1522.115316\n",
      "iteration 400 / 2000: loss 1519.899886\n",
      "iteration 500 / 2000: loss 1516.749973\n",
      "iteration 600 / 2000: loss 1513.442821\n",
      "iteration 700 / 2000: loss 1511.031604\n",
      "iteration 800 / 2000: loss 1507.857781\n",
      "iteration 900 / 2000: loss 1504.507115\n",
      "iteration 1000 / 2000: loss 1502.167640\n",
      "iteration 1100 / 2000: loss 1498.689584\n",
      "iteration 1200 / 2000: loss 1495.807971\n",
      "iteration 1300 / 2000: loss 1492.868977\n",
      "iteration 1400 / 2000: loss 1489.671974\n",
      "iteration 1500 / 2000: loss 1486.443170\n",
      "iteration 1600 / 2000: loss 1483.922677\n",
      "iteration 1700 / 2000: loss 1480.667472\n",
      "iteration 1800 / 2000: loss 1477.942224\n",
      "iteration 1900 / 2000: loss 1474.123135\n",
      "iteration 0 / 2000: loss 15454.130312\n",
      "iteration 100 / 2000: loss 15148.124127\n",
      "iteration 200 / 2000: loss 14848.438801\n",
      "iteration 300 / 2000: loss 14553.663880\n",
      "iteration 400 / 2000: loss 14266.047821\n",
      "iteration 500 / 2000: loss 13983.819137\n",
      "iteration 600 / 2000: loss 13706.956180\n",
      "iteration 700 / 2000: loss 13435.341458\n",
      "iteration 800 / 2000: loss 13169.367838\n",
      "iteration 900 / 2000: loss 12908.501607\n",
      "iteration 1000 / 2000: loss 12652.908282\n",
      "iteration 1100 / 2000: loss 12402.524958\n",
      "iteration 1200 / 2000: loss 12156.587935\n",
      "iteration 1300 / 2000: loss 11916.336355\n",
      "iteration 1400 / 2000: loss 11680.488127\n",
      "iteration 1500 / 2000: loss 11448.496387\n",
      "iteration 1600 / 2000: loss 11222.072459\n",
      "iteration 1700 / 2000: loss 10999.718705\n",
      "iteration 1800 / 2000: loss 10782.294224\n",
      "iteration 1900 / 2000: loss 10568.582683\n",
      "iteration 0 / 2000: loss 4.994884\n",
      "iteration 100 / 2000: loss 4.888826\n",
      "iteration 200 / 2000: loss 4.603331\n",
      "iteration 300 / 2000: loss 4.251510\n",
      "iteration 400 / 2000: loss 4.525186\n",
      "iteration 500 / 2000: loss 4.356789\n",
      "iteration 600 / 2000: loss 4.063105\n",
      "iteration 700 / 2000: loss 4.021117\n",
      "iteration 800 / 2000: loss 3.887384\n",
      "iteration 900 / 2000: loss 3.718073\n",
      "iteration 1000 / 2000: loss 3.913058\n",
      "iteration 1100 / 2000: loss 3.668896\n",
      "iteration 1200 / 2000: loss 3.710458\n",
      "iteration 1300 / 2000: loss 3.286086\n",
      "iteration 1400 / 2000: loss 3.796403\n",
      "iteration 1500 / 2000: loss 3.399272\n",
      "iteration 1600 / 2000: loss 3.345611\n",
      "iteration 1700 / 2000: loss 3.465066\n",
      "iteration 1800 / 2000: loss 3.281923\n",
      "iteration 1900 / 2000: loss 3.267403\n",
      "iteration 0 / 2000: loss 4.950319\n",
      "iteration 100 / 2000: loss 4.680887\n",
      "iteration 200 / 2000: loss 4.599518\n",
      "iteration 300 / 2000: loss 4.664134\n",
      "iteration 400 / 2000: loss 3.749423\n",
      "iteration 500 / 2000: loss 4.495075\n",
      "iteration 600 / 2000: loss 3.726814\n",
      "iteration 700 / 2000: loss 3.952452\n",
      "iteration 800 / 2000: loss 3.822894\n",
      "iteration 900 / 2000: loss 3.788402\n",
      "iteration 1000 / 2000: loss 3.685085\n",
      "iteration 1100 / 2000: loss 3.574730\n",
      "iteration 1200 / 2000: loss 3.653957\n",
      "iteration 1300 / 2000: loss 3.588697\n",
      "iteration 1400 / 2000: loss 3.377460\n",
      "iteration 1500 / 2000: loss 3.515335\n",
      "iteration 1600 / 2000: loss 3.210669\n",
      "iteration 1700 / 2000: loss 3.300674\n",
      "iteration 1800 / 2000: loss 3.510452\n",
      "iteration 1900 / 2000: loss 3.441604\n",
      "iteration 0 / 2000: loss 5.313021\n",
      "iteration 100 / 2000: loss 4.745452\n",
      "iteration 200 / 2000: loss 4.676499\n",
      "iteration 300 / 2000: loss 4.470428\n",
      "iteration 400 / 2000: loss 3.837129\n",
      "iteration 500 / 2000: loss 3.665611\n",
      "iteration 600 / 2000: loss 4.103267\n",
      "iteration 700 / 2000: loss 4.093494\n",
      "iteration 800 / 2000: loss 3.840778\n",
      "iteration 900 / 2000: loss 3.587180\n",
      "iteration 1000 / 2000: loss 3.440833\n",
      "iteration 1100 / 2000: loss 3.566967\n",
      "iteration 1200 / 2000: loss 3.645409\n",
      "iteration 1300 / 2000: loss 3.228357\n",
      "iteration 1400 / 2000: loss 3.136933\n",
      "iteration 1500 / 2000: loss 3.393666\n",
      "iteration 1600 / 2000: loss 3.587022\n",
      "iteration 1700 / 2000: loss 3.358085\n",
      "iteration 1800 / 2000: loss 3.643167\n",
      "iteration 1900 / 2000: loss 3.577394\n",
      "iteration 0 / 2000: loss 5.663595\n",
      "iteration 100 / 2000: loss 5.669636\n",
      "iteration 200 / 2000: loss 4.966534\n",
      "iteration 300 / 2000: loss 4.586365\n",
      "iteration 400 / 2000: loss 4.316627\n",
      "iteration 500 / 2000: loss 4.209293\n",
      "iteration 600 / 2000: loss 4.051960\n",
      "iteration 700 / 2000: loss 4.070660\n",
      "iteration 800 / 2000: loss 3.683005\n",
      "iteration 900 / 2000: loss 4.151521\n",
      "iteration 1000 / 2000: loss 3.660413\n",
      "iteration 1100 / 2000: loss 3.717954\n",
      "iteration 1200 / 2000: loss 3.708826\n",
      "iteration 1300 / 2000: loss 3.192689\n",
      "iteration 1400 / 2000: loss 3.411598\n",
      "iteration 1500 / 2000: loss 3.636141\n",
      "iteration 1600 / 2000: loss 3.373060\n",
      "iteration 1700 / 2000: loss 3.594607\n",
      "iteration 1800 / 2000: loss 3.312911\n",
      "iteration 1900 / 2000: loss 3.599956\n",
      "iteration 0 / 2000: loss 5.733893\n",
      "iteration 100 / 2000: loss 5.228136\n",
      "iteration 200 / 2000: loss 4.986762\n",
      "iteration 300 / 2000: loss 4.677487\n",
      "iteration 400 / 2000: loss 4.480989\n",
      "iteration 500 / 2000: loss 4.373201\n",
      "iteration 600 / 2000: loss 3.908201\n",
      "iteration 700 / 2000: loss 4.097075\n",
      "iteration 800 / 2000: loss 4.584970\n",
      "iteration 900 / 2000: loss 4.048061\n",
      "iteration 1000 / 2000: loss 3.692679\n",
      "iteration 1100 / 2000: loss 3.587175\n",
      "iteration 1200 / 2000: loss 4.233872\n",
      "iteration 1300 / 2000: loss 3.967991\n",
      "iteration 1400 / 2000: loss 3.687032\n",
      "iteration 1500 / 2000: loss 3.723533\n",
      "iteration 1600 / 2000: loss 3.616717\n",
      "iteration 1700 / 2000: loss 3.731367\n",
      "iteration 1800 / 2000: loss 3.724073\n",
      "iteration 1900 / 2000: loss 3.611243\n",
      "iteration 0 / 2000: loss 7.613585\n",
      "iteration 100 / 2000: loss 7.508633\n",
      "iteration 200 / 2000: loss 7.066546\n",
      "iteration 300 / 2000: loss 6.274743\n",
      "iteration 400 / 2000: loss 6.072239\n",
      "iteration 500 / 2000: loss 5.839437\n",
      "iteration 600 / 2000: loss 6.137580\n",
      "iteration 700 / 2000: loss 5.594034\n",
      "iteration 800 / 2000: loss 5.821391\n",
      "iteration 900 / 2000: loss 5.851224\n",
      "iteration 1000 / 2000: loss 5.362120\n",
      "iteration 1100 / 2000: loss 5.276686\n",
      "iteration 1200 / 2000: loss 5.327911\n",
      "iteration 1300 / 2000: loss 5.163211\n",
      "iteration 1400 / 2000: loss 5.053780\n",
      "iteration 1500 / 2000: loss 5.021123\n",
      "iteration 1600 / 2000: loss 5.428611\n",
      "iteration 1700 / 2000: loss 4.997190\n",
      "iteration 1800 / 2000: loss 5.107165\n",
      "iteration 1900 / 2000: loss 4.861046\n",
      "iteration 0 / 2000: loss 20.855095\n",
      "iteration 100 / 2000: loss 20.573253\n",
      "iteration 200 / 2000: loss 19.988461\n",
      "iteration 300 / 2000: loss 20.013677\n",
      "iteration 400 / 2000: loss 19.039387\n",
      "iteration 500 / 2000: loss 18.955705\n",
      "iteration 600 / 2000: loss 18.743667\n",
      "iteration 700 / 2000: loss 18.603253\n",
      "iteration 800 / 2000: loss 18.553872\n",
      "iteration 900 / 2000: loss 18.051676\n",
      "iteration 1000 / 2000: loss 18.259082\n",
      "iteration 1100 / 2000: loss 18.373867\n",
      "iteration 1200 / 2000: loss 18.121015\n",
      "iteration 1300 / 2000: loss 17.771436\n",
      "iteration 1400 / 2000: loss 18.014009\n",
      "iteration 1500 / 2000: loss 18.112220\n",
      "iteration 1600 / 2000: loss 17.570248\n",
      "iteration 1700 / 2000: loss 17.533064\n",
      "iteration 1800 / 2000: loss 17.517363\n",
      "iteration 1900 / 2000: loss 17.431518\n",
      "iteration 0 / 2000: loss 160.360082\n",
      "iteration 100 / 2000: loss 155.111317\n",
      "iteration 200 / 2000: loss 149.368714\n",
      "iteration 300 / 2000: loss 144.425263\n",
      "iteration 400 / 2000: loss 139.843306\n",
      "iteration 500 / 2000: loss 134.616791\n",
      "iteration 600 / 2000: loss 130.646773\n",
      "iteration 700 / 2000: loss 126.098365\n",
      "iteration 800 / 2000: loss 122.281511\n",
      "iteration 900 / 2000: loss 117.882169\n",
      "iteration 1000 / 2000: loss 114.148759\n",
      "iteration 1100 / 2000: loss 110.145053\n",
      "iteration 1200 / 2000: loss 106.623277\n",
      "iteration 1300 / 2000: loss 103.197231\n",
      "iteration 1400 / 2000: loss 99.913120\n",
      "iteration 1500 / 2000: loss 96.771969\n",
      "iteration 1600 / 2000: loss 93.548454\n",
      "iteration 1700 / 2000: loss 90.456861\n",
      "iteration 1800 / 2000: loss 87.376970\n",
      "iteration 1900 / 2000: loss 84.475088\n",
      "iteration 0 / 2000: loss 1550.176489\n",
      "iteration 100 / 2000: loss 1110.304573\n",
      "iteration 200 / 2000: loss 795.453985\n",
      "iteration 300 / 2000: loss 569.818263\n",
      "iteration 400 / 2000: loss 408.443458\n",
      "iteration 500 / 2000: loss 292.898342\n",
      "iteration 600 / 2000: loss 210.353547\n",
      "iteration 700 / 2000: loss 151.110162\n",
      "iteration 800 / 2000: loss 108.839808\n",
      "iteration 900 / 2000: loss 78.496253\n",
      "iteration 1000 / 2000: loss 56.739754\n",
      "iteration 1100 / 2000: loss 41.211714\n",
      "iteration 1200 / 2000: loss 30.163735\n",
      "iteration 1300 / 2000: loss 22.162911\n",
      "iteration 1400 / 2000: loss 16.469252\n",
      "iteration 1500 / 2000: loss 12.402636\n",
      "iteration 1600 / 2000: loss 9.488195\n",
      "iteration 1700 / 2000: loss 7.451060\n",
      "iteration 1800 / 2000: loss 5.877309\n",
      "iteration 1900 / 2000: loss 4.810347\n",
      "iteration 0 / 2000: loss 15287.861590\n",
      "iteration 100 / 2000: loss 530.481177\n",
      "iteration 200 / 2000: loss 20.521800\n",
      "iteration 300 / 2000: loss 2.886673\n",
      "iteration 400 / 2000: loss 2.287291\n",
      "iteration 500 / 2000: loss 2.274114\n",
      "iteration 600 / 2000: loss 2.260476\n",
      "iteration 700 / 2000: loss 2.254552\n",
      "iteration 800 / 2000: loss 2.275745\n",
      "iteration 900 / 2000: loss 2.267100\n",
      "iteration 1000 / 2000: loss 2.267728\n",
      "iteration 1100 / 2000: loss 2.259414\n",
      "iteration 1200 / 2000: loss 2.276679\n",
      "iteration 1300 / 2000: loss 2.268847\n",
      "iteration 1400 / 2000: loss 2.261161\n",
      "iteration 1500 / 2000: loss 2.261716\n",
      "iteration 1600 / 2000: loss 2.262821\n",
      "iteration 1700 / 2000: loss 2.261258\n",
      "iteration 1800 / 2000: loss 2.271467\n",
      "iteration 1900 / 2000: loss 2.265341\n",
      "iteration 0 / 2000: loss 6.526961\n",
      "iteration 100 / 2000: loss 2.561276\n",
      "iteration 200 / 2000: loss 2.146313\n",
      "iteration 300 / 2000: loss 2.152425\n",
      "iteration 400 / 2000: loss 1.971131\n",
      "iteration 500 / 2000: loss 2.040707\n",
      "iteration 600 / 2000: loss 2.133650\n",
      "iteration 700 / 2000: loss 1.715402\n",
      "iteration 800 / 2000: loss 2.017705\n",
      "iteration 900 / 2000: loss 1.842291\n",
      "iteration 1000 / 2000: loss 1.712866\n",
      "iteration 1100 / 2000: loss 1.748036\n",
      "iteration 1200 / 2000: loss 1.859985\n",
      "iteration 1300 / 2000: loss 2.035843\n",
      "iteration 1400 / 2000: loss 1.659183\n",
      "iteration 1500 / 2000: loss 1.892002\n",
      "iteration 1600 / 2000: loss 1.814684\n",
      "iteration 1700 / 2000: loss 1.646834\n",
      "iteration 1800 / 2000: loss 1.598075\n",
      "iteration 1900 / 2000: loss 1.813184\n",
      "iteration 0 / 2000: loss 5.339853\n",
      "iteration 100 / 2000: loss 2.298989\n",
      "iteration 200 / 2000: loss 2.262471\n",
      "iteration 300 / 2000: loss 2.008101\n",
      "iteration 400 / 2000: loss 1.941608\n",
      "iteration 500 / 2000: loss 2.029447\n",
      "iteration 600 / 2000: loss 1.898953\n",
      "iteration 700 / 2000: loss 2.048195\n",
      "iteration 800 / 2000: loss 1.815329\n",
      "iteration 900 / 2000: loss 1.985258\n",
      "iteration 1000 / 2000: loss 1.855030\n",
      "iteration 1100 / 2000: loss 1.763891\n",
      "iteration 1200 / 2000: loss 1.814964\n",
      "iteration 1300 / 2000: loss 1.859845\n",
      "iteration 1400 / 2000: loss 1.957940\n",
      "iteration 1500 / 2000: loss 1.989763\n",
      "iteration 1600 / 2000: loss 1.702803\n",
      "iteration 1700 / 2000: loss 1.908674\n",
      "iteration 1800 / 2000: loss 1.539260\n",
      "iteration 1900 / 2000: loss 1.823623\n",
      "iteration 0 / 2000: loss 5.846876\n",
      "iteration 100 / 2000: loss 2.566684\n",
      "iteration 200 / 2000: loss 2.406653\n",
      "iteration 300 / 2000: loss 1.999958\n",
      "iteration 400 / 2000: loss 1.845613\n",
      "iteration 500 / 2000: loss 1.972637\n",
      "iteration 600 / 2000: loss 1.939957\n",
      "iteration 700 / 2000: loss 1.940364\n",
      "iteration 800 / 2000: loss 1.999763\n",
      "iteration 900 / 2000: loss 1.891068\n",
      "iteration 1000 / 2000: loss 1.991637\n",
      "iteration 1100 / 2000: loss 1.853750\n",
      "iteration 1200 / 2000: loss 1.779136\n",
      "iteration 1300 / 2000: loss 1.776283\n",
      "iteration 1400 / 2000: loss 1.905619\n",
      "iteration 1500 / 2000: loss 1.980235\n",
      "iteration 1600 / 2000: loss 1.918123\n",
      "iteration 1700 / 2000: loss 1.776876\n",
      "iteration 1800 / 2000: loss 1.786893\n",
      "iteration 1900 / 2000: loss 1.727240\n",
      "iteration 0 / 2000: loss 5.121384\n",
      "iteration 100 / 2000: loss 2.203140\n",
      "iteration 200 / 2000: loss 2.272682\n",
      "iteration 300 / 2000: loss 2.297342\n",
      "iteration 400 / 2000: loss 1.965387\n",
      "iteration 500 / 2000: loss 2.236188\n",
      "iteration 600 / 2000: loss 1.951826\n",
      "iteration 700 / 2000: loss 1.967493\n",
      "iteration 800 / 2000: loss 2.077472\n",
      "iteration 900 / 2000: loss 1.892021\n",
      "iteration 1000 / 2000: loss 1.726191\n",
      "iteration 1100 / 2000: loss 1.690266\n",
      "iteration 1200 / 2000: loss 1.621542\n",
      "iteration 1300 / 2000: loss 1.809604\n",
      "iteration 1400 / 2000: loss 2.002475\n",
      "iteration 1500 / 2000: loss 1.817523\n",
      "iteration 1600 / 2000: loss 1.835220\n",
      "iteration 1700 / 2000: loss 1.829619\n",
      "iteration 1800 / 2000: loss 1.807581\n",
      "iteration 1900 / 2000: loss 1.923165\n",
      "iteration 0 / 2000: loss 6.356300\n",
      "iteration 100 / 2000: loss 2.683801\n",
      "iteration 200 / 2000: loss 2.194569\n",
      "iteration 300 / 2000: loss 2.151916\n",
      "iteration 400 / 2000: loss 2.093139\n",
      "iteration 500 / 2000: loss 2.157258\n",
      "iteration 600 / 2000: loss 2.044096\n",
      "iteration 700 / 2000: loss 2.179663\n",
      "iteration 800 / 2000: loss 2.088960\n",
      "iteration 900 / 2000: loss 1.924110\n",
      "iteration 1000 / 2000: loss 1.973429\n",
      "iteration 1100 / 2000: loss 1.961898\n",
      "iteration 1200 / 2000: loss 2.011475\n",
      "iteration 1300 / 2000: loss 1.927930\n",
      "iteration 1400 / 2000: loss 1.832975\n",
      "iteration 1500 / 2000: loss 1.896892\n",
      "iteration 1600 / 2000: loss 1.875950\n",
      "iteration 1700 / 2000: loss 1.824222\n",
      "iteration 1800 / 2000: loss 1.863368\n",
      "iteration 1900 / 2000: loss 1.892670\n",
      "iteration 0 / 2000: loss 6.347111\n",
      "iteration 100 / 2000: loss 3.721327\n",
      "iteration 200 / 2000: loss 3.548884\n",
      "iteration 300 / 2000: loss 3.198834\n",
      "iteration 400 / 2000: loss 3.131584\n",
      "iteration 500 / 2000: loss 3.041115\n",
      "iteration 600 / 2000: loss 2.830370\n",
      "iteration 700 / 2000: loss 2.862121\n",
      "iteration 800 / 2000: loss 2.898536\n",
      "iteration 900 / 2000: loss 2.526857\n",
      "iteration 1000 / 2000: loss 2.705174\n",
      "iteration 1100 / 2000: loss 2.544505\n",
      "iteration 1200 / 2000: loss 2.458444\n",
      "iteration 1300 / 2000: loss 2.436305\n",
      "iteration 1400 / 2000: loss 2.426181\n",
      "iteration 1500 / 2000: loss 2.408997\n",
      "iteration 1600 / 2000: loss 2.338156\n",
      "iteration 1700 / 2000: loss 2.243625\n",
      "iteration 1800 / 2000: loss 2.247786\n",
      "iteration 1900 / 2000: loss 2.145801\n",
      "iteration 0 / 2000: loss 20.733779\n",
      "iteration 100 / 2000: loss 10.950603\n",
      "iteration 200 / 2000: loss 6.924166\n",
      "iteration 300 / 2000: loss 4.469550\n",
      "iteration 400 / 2000: loss 3.379008\n",
      "iteration 500 / 2000: loss 2.730531\n",
      "iteration 600 / 2000: loss 2.278072\n",
      "iteration 700 / 2000: loss 2.177416\n",
      "iteration 800 / 2000: loss 1.969262\n",
      "iteration 900 / 2000: loss 1.810440\n",
      "iteration 1000 / 2000: loss 1.800981\n",
      "iteration 1100 / 2000: loss 1.896276\n",
      "iteration 1200 / 2000: loss 1.902025\n",
      "iteration 1300 / 2000: loss 1.764081\n",
      "iteration 1400 / 2000: loss 1.924637\n",
      "iteration 1500 / 2000: loss 1.817308\n",
      "iteration 1600 / 2000: loss 1.808019\n",
      "iteration 1700 / 2000: loss 1.831117\n",
      "iteration 1800 / 2000: loss 1.839971\n",
      "iteration 1900 / 2000: loss 1.672520\n",
      "iteration 0 / 2000: loss 160.325792\n",
      "iteration 100 / 2000: loss 2.464192\n",
      "iteration 200 / 2000: loss 1.913440\n",
      "iteration 300 / 2000: loss 2.001442\n",
      "iteration 400 / 2000: loss 1.950968\n",
      "iteration 500 / 2000: loss 1.955946\n",
      "iteration 600 / 2000: loss 2.032482\n",
      "iteration 700 / 2000: loss 1.918867\n",
      "iteration 800 / 2000: loss 2.009192\n",
      "iteration 900 / 2000: loss 2.004628\n",
      "iteration 1000 / 2000: loss 1.954745\n",
      "iteration 1100 / 2000: loss 1.867648\n",
      "iteration 1200 / 2000: loss 2.121288\n",
      "iteration 1300 / 2000: loss 1.950871\n",
      "iteration 1400 / 2000: loss 1.961564\n",
      "iteration 1500 / 2000: loss 1.954866\n",
      "iteration 1600 / 2000: loss 2.088703\n",
      "iteration 1700 / 2000: loss 2.059482\n",
      "iteration 1800 / 2000: loss 2.071352\n",
      "iteration 1900 / 2000: loss 1.982889\n",
      "iteration 0 / 2000: loss 1530.646161\n",
      "iteration 100 / 2000: loss 2.187168\n",
      "iteration 200 / 2000: loss 2.191832\n",
      "iteration 300 / 2000: loss 2.220346\n",
      "iteration 400 / 2000: loss 2.234808\n",
      "iteration 500 / 2000: loss 2.152881\n",
      "iteration 600 / 2000: loss 2.199252\n",
      "iteration 700 / 2000: loss 2.219855\n",
      "iteration 800 / 2000: loss 2.150274\n",
      "iteration 900 / 2000: loss 2.281035\n",
      "iteration 1000 / 2000: loss 2.174814\n",
      "iteration 1100 / 2000: loss 2.189538\n",
      "iteration 1200 / 2000: loss 2.172543\n",
      "iteration 1300 / 2000: loss 2.188807\n",
      "iteration 1400 / 2000: loss 2.179678\n",
      "iteration 1500 / 2000: loss 2.227318\n",
      "iteration 1600 / 2000: loss 2.198575\n",
      "iteration 1700 / 2000: loss 2.196280\n",
      "iteration 1800 / 2000: loss 2.184568\n",
      "iteration 1900 / 2000: loss 2.306366\n",
      "iteration 0 / 2000: loss 15237.258179\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.022765\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.023587\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.273734\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.201890\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.500821\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.504086\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 21.482049\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 158.145988\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 1535.515097\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 15390.363922\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.772192\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.956334\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.739078\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.701869\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.188320\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 7.679589\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 20.347459\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 159.517799\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 1543.251938\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 15224.961303\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.343077\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 4.686129\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.005349\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.226446\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.951469\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 7.354290\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 21.976289\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 162.019493\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 1548.174107\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 15455.196667\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.081748\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.747973\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 7.043472\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 7.566378\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.359531\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 8.230831\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 21.223264\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 157.472251\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 1552.753421\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 15415.005179\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.324331\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.927341\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.864133\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.393460\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.741502\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 7.244415\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 21.194937\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 158.451374\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 1563.566184\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 15619.497430\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 4.802929\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.541495\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.113749\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 7.013414\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.224395\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.229595\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 21.427674\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 158.003677\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 1523.283666\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 15470.805410\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.780061\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.661408\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.421094\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.764954\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 5.646869\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 6.703117\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 22.269779\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 157.569340\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 1545.638693\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 15249.751318\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "lr 1.000000e-10 reg 1.000000e-03 train accuracy: 0.091633 val accuracy: 0.081000\n",
      "lr 1.000000e-10 reg 1.000000e-02 train accuracy: 0.108755 val accuracy: 0.113000\n",
      "lr 1.000000e-10 reg 1.000000e-01 train accuracy: 0.087918 val accuracy: 0.082000\n",
      "lr 1.000000e-10 reg 1.000000e+00 train accuracy: 0.115898 val accuracy: 0.094000\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.112490 val accuracy: 0.116000\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.092837 val accuracy: 0.083000\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.111041 val accuracy: 0.096000\n",
      "lr 1.000000e-10 reg 1.000000e+04 train accuracy: 0.093531 val accuracy: 0.097000\n",
      "lr 1.000000e-10 reg 1.000000e+05 train accuracy: 0.097531 val accuracy: 0.089000\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.098245 val accuracy: 0.087000\n",
      "lr 1.668101e-08 reg 1.000000e-03 train accuracy: 0.190959 val accuracy: 0.187000\n",
      "lr 1.668101e-08 reg 1.000000e-02 train accuracy: 0.174429 val accuracy: 0.184000\n",
      "lr 1.668101e-08 reg 1.000000e-01 train accuracy: 0.176102 val accuracy: 0.187000\n",
      "lr 1.668101e-08 reg 1.000000e+00 train accuracy: 0.190816 val accuracy: 0.213000\n",
      "lr 1.668101e-08 reg 1.000000e+01 train accuracy: 0.193367 val accuracy: 0.183000\n",
      "lr 1.668101e-08 reg 1.000000e+02 train accuracy: 0.183041 val accuracy: 0.177000\n",
      "lr 1.668101e-08 reg 1.000000e+03 train accuracy: 0.185918 val accuracy: 0.176000\n",
      "lr 1.668101e-08 reg 1.000000e+04 train accuracy: 0.199306 val accuracy: 0.189000\n",
      "lr 1.668101e-08 reg 1.000000e+05 train accuracy: 0.300224 val accuracy: 0.322000\n",
      "lr 1.668101e-08 reg 1.000000e+06 train accuracy: 0.256939 val accuracy: 0.267000\n",
      "lr 2.782559e-06 reg 1.000000e-03 train accuracy: 0.384286 val accuracy: 0.359000\n",
      "lr 2.782559e-06 reg 1.000000e-02 train accuracy: 0.394898 val accuracy: 0.378000\n",
      "lr 2.782559e-06 reg 1.000000e-01 train accuracy: 0.396633 val accuracy: 0.348000\n",
      "lr 2.782559e-06 reg 1.000000e+00 train accuracy: 0.392265 val accuracy: 0.381000\n",
      "lr 2.782559e-06 reg 1.000000e+01 train accuracy: 0.392367 val accuracy: 0.392000\n",
      "lr 2.782559e-06 reg 1.000000e+02 train accuracy: 0.396204 val accuracy: 0.377000\n",
      "lr 2.782559e-06 reg 1.000000e+03 train accuracy: 0.402469 val accuracy: 0.405000\n",
      "lr 2.782559e-06 reg 1.000000e+04 train accuracy: 0.355551 val accuracy: 0.356000\n",
      "lr 2.782559e-06 reg 1.000000e+05 train accuracy: 0.261857 val accuracy: 0.262000\n",
      "lr 2.782559e-06 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.405000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.logspace(-10, 10, 10)\n",
    "regularization_strengths = np.logspace(-3, 6, 10)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "#pass\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        cls = Softmax()\n",
    "        cls.train(X_train, y_train, learning_rate=lr, reg=reg,num_iters=2000, verbose=True)\n",
    "        y_train_pred = cls.predict(X_train)\n",
    "        y_val_pred = cls.predict(X_val)\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[(lr,reg)] = (train_accuracy, val_accuracy)\n",
    "        if val_accuracy > best_val:\n",
    "            best_val = val_accuracy\n",
    "            best_softmax = cls\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.334000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAF/CAYAAABQVS1eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIBAIBAKBwB34L9/txdqObDcgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7b24208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
